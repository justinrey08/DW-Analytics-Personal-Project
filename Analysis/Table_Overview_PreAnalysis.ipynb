{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_time: timestamp (nullable = true)\n",
      " |-- sales_outlet_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- instore_yn: string (nullable = true)\n",
      " |-- order: integer (nullable = true)\n",
      " |-- line_item_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- line_item_amount: double (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- promo_item_yn: string (nullable = true)\n",
      "\n",
      "49894 14\n",
      "+--------------+----------------+-------------------+---------------+--------+-----------+----------+-----+------------+----------+--------+----------------+----------+-------------+\n",
      "|transaction_id|transaction_date|   transaction_time|sales_outlet_id|staff_id|customer_id|instore_yn|order|line_item_id|product_id|quantity|line_item_amount|unit_price|promo_item_yn|\n",
      "+--------------+----------------+-------------------+---------------+--------+-----------+----------+-----+------------+----------+--------+----------------+----------+-------------+\n",
      "|             7|      2019-04-01|2024-02-05 12:04:43|              3|      12|        558|         N|    1|           1|        52|       1|             2.5|       2.5|            N|\n",
      "|            11|      2019-04-01|2024-02-05 15:54:39|              3|      17|        781|         N|    1|           1|        27|       2|             7.0|       3.5|            N|\n",
      "|            19|      2019-04-01|2024-02-05 14:34:59|              3|      17|        788|         Y|    1|           1|        46|       2|             5.0|       2.5|            N|\n",
      "|            32|      2019-04-01|2024-02-05 16:06:04|              3|      12|        683|         N|    1|           1|        23|       2|             5.0|       2.5|            N|\n",
      "|            33|      2019-04-01|2024-02-05 19:18:37|              3|      17|         99|         Y|    1|           1|        34|       1|            2.45|      2.45|            N|\n",
      "|            39|      2019-04-01|2024-02-05 18:54:46|              3|      17|        664|         Y|    1|           1|        32|       1|             3.0|       3.0|            N|\n",
      "|            50|      2019-04-01|2024-02-05 13:03:49|              3|      12|        316|         N|    1|           1|        49|       2|             6.0|       3.0|            N|\n",
      "|            53|      2019-04-01|2024-02-05 11:21:14|              3|      12|         38|         N|    1|           1|        60|       1|            3.75|      3.75|            N|\n",
      "|            59|      2019-04-01|2024-02-05 19:30:55|              3|      12|        370|         Y|    1|           1|        51|       2|             6.0|       3.0|            N|\n",
      "|            62|      2019-04-01|2024-02-05 12:01:00|              3|      12|        180|         Y|    1|           1|        49|       2|             6.0|       3.0|            N|\n",
      "|            81|      2019-04-01|2024-02-05 13:20:51|              3|      12|         35|         Y|    1|           1|        35|       2|             6.2|       3.1|            N|\n",
      "|            90|      2019-04-01|2024-02-05 11:40:20|              3|      17|        595|         N|    1|           1|        47|       1|             3.0|       3.0|            N|\n",
      "|            91|      2019-04-01|2024-02-05 12:27:43|              3|      12|        500|         N|    1|           1|        25|       2|             4.4|       2.2|            N|\n",
      "|            94|      2019-04-01|2024-02-05 17:37:02|              3|      17|        128|         Y|    1|           1|        23|       2|             5.0|       2.5|            N|\n",
      "|           101|      2019-04-01|2024-02-05 12:26:34|              3|      17|        599|         Y|    1|           1|        48|       2|             5.0|       2.5|            N|\n",
      "|           103|      2019-04-01|2024-02-05 14:33:32|              3|      17|        152|         Y|    1|           1|        53|       2|             6.0|       3.0|            N|\n",
      "|           108|      2019-04-01|2024-02-05 16:11:18|              3|      17|         65|         N|    1|           1|        40|       1|            3.75|      3.75|            N|\n",
      "|           112|      2019-04-01|2024-02-05 13:03:38|              3|      12|         90|         Y|    1|           1|        37|       2|             6.0|       3.0|            N|\n",
      "|           117|      2019-04-01|2024-02-05 18:25:04|              3|      12|         82|         Y|    1|           1|        60|       1|            3.75|      3.75|            N|\n",
      "|           120|      2019-04-01|2024-02-05 15:51:44|              3|      12|        174|         N|    1|           1|        53|       2|             6.0|       3.0|            N|\n",
      "+--------------+----------------+-------------------+---------------+--------+-----------+----------+-----+------------+----------+--------+----------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales Receipts\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Sales Receipt CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\201904 sales reciepts.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "sales_receipt_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "sales_receipt_data.printSchema() \n",
    "cd_row = sales_receipt_data.count()\n",
    "cd_cols = len(sales_receipt_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "sales_receipt_data.show()\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- home_store: integer (nullable = true)\n",
      " |-- customer_first-name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_since: date (nullable = true)\n",
      " |-- loyalty_card_number: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      "\n",
      "2246 9\n",
      "+-----------+----------+-------------------+--------------------+--------------+-------------------+----------+------+----------+\n",
      "|customer_id|home_store|customer_first-name|      customer_email|customer_since|loyalty_card_number| birthdate|gender|birth_year|\n",
      "+-----------+----------+-------------------+--------------------+--------------+-------------------+----------+------+----------+\n",
      "|          1|         3|          Kelly Key|Venus@adipiscing.edu|    2017-01-04|       908-424-2890|1950-05-29|     M|      1950|\n",
      "|          2|         3|    Clark Schroeder|      Nora@fames.gov|    2017-01-07|       032-732-6308|1950-07-30|     M|      1950|\n",
      "|          3|         3|     Elvis Cardenas|  Brianna@tellus.edu|    2017-01-10|       459-375-9187|1950-09-30|     M|      1950|\n",
      "|          4|         3|       Rafael Estes|         Ina@non.gov|    2017-01-13|       576-640-9226|1950-12-01|     M|      1950|\n",
      "|          5|         3|         Colin Lynn|    Dale@Integer.com|    2017-01-15|       344-674-6569|1951-02-01|     M|      1951|\n",
      "|          6|         3|         Igor Beach|     Caleb@morbi.net|    2017-01-18|       114-126-1992|1951-04-04|     M|      1951|\n",
      "|          7|         3|       Scott Holden|     Yen@Integer.edu|    2017-01-21|       384-074-3606|1951-06-05|     M|      1951|\n",
      "|          8|         3|       Keegan Ayala|     Tana@sociis.com|    2017-01-24|       257-308-7675|1951-08-07|     M|      1951|\n",
      "|          9|         3|         Amir Byers|Madeson@malesuada.us|    2017-01-26|       931-925-0273|1951-10-08|     M|      1951|\n",
      "|         10|         3|       Magee Malone|  Anjolie@sapien.gov|    2017-01-29|       359-150-6747|1951-12-09|     M|      1951|\n",
      "|         11|         3|        Dolan Petty|   Kim@convallis.edu|    2017-02-01|       547-881-4488|1952-02-09|     M|      1952|\n",
      "|         12|         3|        Wang Hebert|    Hollee@lorem.net|    2017-02-04|       605-014-6218|1952-04-11|     M|      1952|\n",
      "|         13|         3|        Wayne David|    Slade@sapien.edu|    2017-02-06|       430-935-8698|1952-06-12|     M|      1952|\n",
      "|         14|         3|    Colin Fernandez|    Thaddeus@non.com|    2017-02-09|       703-178-5883|1952-08-13|     M|      1952|\n",
      "|         15|         3| Holmes Blankenship|  Marvin@accumsan.us|    2017-02-12|       959-439-4036|1952-10-15|     M|      1952|\n",
      "|         16|         3|         Ali French|    Uriah@magnis.org|    2017-02-15|       169-057-5893|1952-12-16|     M|      1952|\n",
      "|         17|         3|      Josiah Burton|    Sacha@Nullam.edu|    2017-02-17|       373-811-5424|1953-02-16|     M|      1953|\n",
      "|         18|         3|       Drew Skinner| Rashad@faucibus.net|    2017-02-20|       981-655-9539|1953-04-19|     M|      1953|\n",
      "|         19|         3|       Henry Wilcox|     Dexter@nisl.gov|    2017-02-23|       833-172-4578|1953-06-20|     M|      1953|\n",
      "|         20|         3|       Nathan Riggs|     Iris@aptent.com|    2017-02-26|       639-354-7929|1953-08-21|     M|      1953|\n",
      "+-----------+----------+-------------------+--------------------+--------------+-------------------+----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customer\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Customer CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\customer.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "cust_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "cust_data.printSchema() \n",
    "cd_row = cust_data.count()\n",
    "cd_cols = len(cust_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "cust_data.show()\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count for DataFrame[customer_id: int] has more than 20. here's the figure: 2246\n",
      "+----------+\n",
      "|home_store|\n",
      "+----------+\n",
      "|         3|\n",
      "|         5|\n",
      "|         8|\n",
      "+----------+\n",
      "\n",
      "Distinct count for DataFrame[home_store: int]: 3\n",
      "Distinct count for DataFrame[customer_first-name: string] has more than 20. here's the figure: 1640\n",
      "Distinct count for DataFrame[customer_email: string] has more than 20. here's the figure: 2246\n",
      "Distinct count for DataFrame[customer_since: date] has more than 20. here's the figure: 794\n",
      "Distinct count for DataFrame[loyalty_card_number: string] has more than 20. here's the figure: 2246\n",
      "Distinct count for DataFrame[birthdate: date] has more than 20. here's the figure: 1883\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "|     N|\n",
      "+------+\n",
      "\n",
      "Distinct count for DataFrame[gender: string]: 3\n",
      "Distinct count for DataFrame[birth_year: int] has more than 20. here's the figure: 52\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at each column and see what and how many the distinct values are at each\n",
    "# Get columns in cust data\n",
    "columns_for_iteration = []\n",
    "for column in cust_data.columns:\n",
    "    column_name = cust_data.select(column).alias(column)\n",
    "    columns_for_iteration.append(column_name)\n",
    "for column_names in columns_for_iteration:\n",
    "    dist_count = column_names.distinct().count()\n",
    "    if dist_count < 20:\n",
    "        column_names.distinct().show()\n",
    "        print(f\"Distinct count for {column_names}:\" , dist_count)\n",
    "    else:\n",
    "        print(f\"Distinct count for {column_names} has more than 20. here's the figure:\" , dist_count)\n",
    "#cust_data['home_store'].distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- Date_ID: integer (nullable = true)\n",
      " |-- Week_ID: integer (nullable = true)\n",
      " |-- Week_Desc: string (nullable = true)\n",
      " |-- Month_ID: integer (nullable = true)\n",
      " |-- Month_Name: string (nullable = true)\n",
      " |-- Quarter_ID: integer (nullable = true)\n",
      " |-- Quarter_Name: string (nullable = true)\n",
      " |-- Year_ID: integer (nullable = true)\n",
      "\n",
      "30 9\n",
      "+----------------+--------+-------+---------+--------+----------+----------+------------+-------+\n",
      "|transaction_date| Date_ID|Week_ID|Week_Desc|Month_ID|Month_Name|Quarter_ID|Quarter_Name|Year_ID|\n",
      "+----------------+--------+-------+---------+--------+----------+----------+------------+-------+\n",
      "|        4/1/2019|20190401|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/2/2019|20190402|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/3/2019|20190403|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/4/2019|20190404|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/5/2019|20190405|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/6/2019|20190406|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/7/2019|20190407|     14|  Week 14|       4|     April|         2|          Q2|   2019|\n",
      "|        4/8/2019|20190408|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|        4/9/2019|20190409|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/10/2019|20190410|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/11/2019|20190411|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/12/2019|20190412|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/13/2019|20190413|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/14/2019|20190414|     15|  Week 15|       4|     April|         2|          Q2|   2019|\n",
      "|       4/15/2019|20190415|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "|       4/16/2019|20190416|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "|       4/17/2019|20190417|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "|       4/18/2019|20190418|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "|       4/19/2019|20190419|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "|       4/20/2019|20190420|     16|  Week 16|       4|     April|         2|          Q2|   2019|\n",
      "+----------------+--------+-------+---------+--------+----------+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dates csv\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read date CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\Dates.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "date_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "date_data.printSchema() \n",
    "cd_row = date_data.count()\n",
    "cd_cols = len(date_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "date_data.show()\n",
    "# Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- generation: string (nullable = true)\n",
      "\n",
      "70 2\n",
      "+----------+------------+\n",
      "|birth_year|  generation|\n",
      "+----------+------------+\n",
      "|      1946|Baby Boomers|\n",
      "|      1947|Baby Boomers|\n",
      "|      1948|Baby Boomers|\n",
      "|      1949|Baby Boomers|\n",
      "|      1950|Baby Boomers|\n",
      "|      1951|Baby Boomers|\n",
      "|      1952|Baby Boomers|\n",
      "|      1953|Baby Boomers|\n",
      "|      1954|Baby Boomers|\n",
      "|      1955|Baby Boomers|\n",
      "|      1956|Baby Boomers|\n",
      "|      1957|Baby Boomers|\n",
      "|      1958|Baby Boomers|\n",
      "|      1959|Baby Boomers|\n",
      "|      1960|Baby Boomers|\n",
      "|      1961|Baby Boomers|\n",
      "|      1962|Baby Boomers|\n",
      "|      1963|Baby Boomers|\n",
      "|      1964|Baby Boomers|\n",
      "|      1965|       Gen X|\n",
      "+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generations csv\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Generations CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\generations.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "Generations_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "Generations_data.printSchema() \n",
    "cd_row = Generations_data.count()\n",
    "cd_cols = len(Generations_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "Generations_data.show()\n",
    "# Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_outlet_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- start_of_day: integer (nullable = true)\n",
      " |-- quantity_sold: integer (nullable = true)\n",
      " |-- waste: integer (nullable = true)\n",
      " |-- % waste: string (nullable = true)\n",
      "\n",
      "307 7\n",
      "+---------------+----------------+----------+------------+-------------+-----+-------+\n",
      "|sales_outlet_id|transaction_date|product_id|start_of_day|quantity_sold|waste|% waste|\n",
      "+---------------+----------------+----------+------------+-------------+-----+-------+\n",
      "|              3|        4/1/2019|        69|          18|            8|   10|    56%|\n",
      "|              3|        4/1/2019|        70|          18|           12|    6|    33%|\n",
      "|              3|        4/1/2019|        71|          18|            8|   10|    56%|\n",
      "|              3|        4/1/2019|        72|          48|            9|   39|    81%|\n",
      "|              3|        4/1/2019|        73|          18|            9|    9|    50%|\n",
      "|              3|        4/2/2019|        69|          18|            7|   11|    61%|\n",
      "|              3|        4/2/2019|        70|          18|           10|    8|    44%|\n",
      "|              3|        4/2/2019|        71|          18|           10|    8|    44%|\n",
      "|              3|        4/2/2019|        72|          48|           10|   38|    79%|\n",
      "|              3|        4/2/2019|        73|          18|            9|    9|    50%|\n",
      "|              3|        4/3/2019|        69|          18|            8|   10|    56%|\n",
      "|              3|        4/3/2019|        70|          18|            5|   13|    72%|\n",
      "|              3|        4/3/2019|        71|          18|            4|   14|    78%|\n",
      "|              3|        4/3/2019|        72|          48|            9|   39|    81%|\n",
      "|              3|        4/3/2019|        73|          18|           15|    3|    17%|\n",
      "|              3|        4/4/2019|        69|          18|           10|    8|    44%|\n",
      "|              3|        4/4/2019|        70|          18|            9|    9|    50%|\n",
      "|              3|        4/4/2019|        71|          18|           10|    8|    44%|\n",
      "|              3|        4/4/2019|        72|          48|           10|   38|    79%|\n",
      "|              3|        4/4/2019|        73|          18|            9|    9|    50%|\n",
      "+---------------+----------------+----------+------------+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pastry Inventory csv\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Pastry Inventory CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\pastry inventory.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "pastry_inventory_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "pastry_inventory_data.printSchema() \n",
    "cd_row = pastry_inventory_data.count()\n",
    "cd_cols = len(pastry_inventory_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "pastry_inventory_data.show()\n",
    "# Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_group: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- unit_of_measure: string (nullable = true)\n",
      " |-- current_wholesale_price: double (nullable = true)\n",
      " |-- current_retail_price: string (nullable = true)\n",
      " |-- tax_exempt_yn: string (nullable = true)\n",
      " |-- promo_yn: string (nullable = true)\n",
      " |-- new_product_yn: string (nullable = true)\n",
      "\n",
      "88 12\n",
      "+----------+---------------+------------------+------------------+--------------------+--------------------+---------------+-----------------------+--------------------+-------------+--------+--------------+\n",
      "|product_id|  product_group|  product_category|      product_type|             product| product_description|unit_of_measure|current_wholesale_price|current_retail_price|tax_exempt_yn|promo_yn|new_product_yn|\n",
      "+----------+---------------+------------------+------------------+--------------------+--------------------+---------------+-----------------------+--------------------+-------------+--------+--------------+\n",
      "|         1|Whole Bean/Teas|      Coffee beans|     Organic Beans| Brazilian - Organic|It's like Carniva...|          12 oz|                   14.4|             $18.00 |            Y|       N|             N|\n",
      "|         2|Whole Bean/Teas|      Coffee beans| House blend Beans|Our Old Time Dine...|Out packed blend ...|          12 oz|                   14.4|             $18.00 |            Y|       N|             N|\n",
      "|         3|Whole Bean/Teas|      Coffee beans|    Espresso Beans|      Espresso Roast|Our house blend f...|           1 lb|                   11.8|             $14.75 |            Y|       N|             N|\n",
      "|         4|Whole Bean/Teas|      Coffee beans|    Espresso Beans|Primo Espresso Roast|Our primium singl...|           1 lb|                  16.36|             $20.45 |            Y|       N|             N|\n",
      "|         5|Whole Bean/Teas|      Coffee beans|     Gourmet Beans|Columbian Medium ...|A smooth cup of c...|           1 lb|                   12.0|             $15.00 |            Y|       N|             N|\n",
      "|         6|Whole Bean/Teas|      Coffee beans|     Gourmet Beans|            Ethiopia|From the home of ...|           1 lb|                   16.8|             $21.00 |            Y|       N|             N|\n",
      "|         7|Whole Bean/Teas|      Coffee beans|     Premium Beans|Jamacian Coffee R...|Ya man, it will s...|           1 lb|                   15.8|             $19.75 |            Y|       N|             N|\n",
      "|         8|Whole Bean/Teas|      Coffee beans|     Premium Beans|           Civet Cat|The most expensiv...|          .5 lb|                   36.0|             $45.00 |            Y|       N|             N|\n",
      "|         9|Whole Bean/Teas|      Coffee beans|     Organic Beans| Organic Decaf Blend|Our blend of hand...|           1 lb|                   18.0|             $22.50 |            Y|       N|             N|\n",
      "|        10|Whole Bean/Teas|      Coffee beans|       Green beans|Guatemalan Sustai...|Green beans you c...|           1 lb|                    8.0|             $10.00 |            Y|       N|             N|\n",
      "|        11|Whole Bean/Teas|         Loose Tea|        Herbal tea|         Lemon Grass|You will think yo...|          .9 oz|                   7.16|              $8.95 |            Y|       N|             N|\n",
      "|        12|Whole Bean/Teas|         Loose Tea|        Herbal tea|          Peppermint|Cool and refreshi...|          .9 oz|                   7.16|              $8.95 |            Y|       N|             N|\n",
      "|        13|Whole Bean/Teas|         Loose Tea|         Black tea|   English Breakfast|The traditional c...|          .9 oz|                   7.16|              $8.95 |            Y|       N|             N|\n",
      "|        14|Whole Bean/Teas|         Loose Tea|         Black tea|           Earl Grey|A full leaf of Or...|          .9 oz|                   7.16|              $8.95 |            Y|       N|             N|\n",
      "|        15|Whole Bean/Teas|         Loose Tea|         Green tea|  Serenity Green Tea|Mountain grown an...|           1 oz|                    7.4|              $9.25 |            Y|       N|             N|\n",
      "|        16|Whole Bean/Teas|         Loose Tea|          Chai tea|Traditional Blend...|A traditional blend.|          .9 oz|                   7.16|              $8.95 |            Y|       N|             N|\n",
      "|        17|Whole Bean/Teas|         Loose Tea|          Chai tea|Morning Sunrise Chai|Fair trade and or...|          .9 oz|                    7.6|              $9.50 |            Y|       N|             N|\n",
      "|        18|Whole Bean/Teas|         Loose Tea|          Chai tea|Spicy Eye Opener ...|A spicier blend t...|          .9 oz|                   8.76|             $10.95 |            Y|       N|             N|\n",
      "|        19|Whole Bean/Teas|Packaged Chocolate|Drinking Chocolate|      Dark chocolate|This drinking cho...|           1 lb|                   5.12|              $6.40 |            Y|       N|             N|\n",
      "|        20|Whole Bean/Teas|Packaged Chocolate| Organic Chocolate|Sustainably Grown...|Certified organic...|           1 lb|                   6.08|              $7.60 |            Y|       N|             N|\n",
      "+----------+---------------+------------------+------------------+--------------------+--------------------+---------------+-----------------------+--------------------+-------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Product csv\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Product Inventory CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\product.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "product_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "product_data.printSchema() \n",
    "cd_row = product_data.count()\n",
    "cd_cols = len(product_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "product_data.show()\n",
    "# Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "55 8\n",
      "+--------+----------+---------+---------------+----------+--------+----+----+\n",
      "|staff_id|first_name|last_name|       position|start_date|location| _c6| _c7|\n",
      "+--------+----------+---------+---------------+----------+--------+----+----+\n",
      "|       1|       Sue|  Tindale|            CFO|  8/3/2001|      HQ|NULL|NULL|\n",
      "|       2|       Ian|  Tindale|            CEO|  8/3/2001|      HQ|NULL|NULL|\n",
      "|       3|     Marny| Hermione|        Roaster|10/24/2007|      WH|NULL|NULL|\n",
      "|       4|   Chelsea|  Claudia|        Roaster|  7/3/2003|      WH|NULL|NULL|\n",
      "|       5|      Alec|  Isadora|        Roaster|  4/2/2008|      WH|NULL|NULL|\n",
      "|       6|      Xena|    Rahim|  Store Manager| 7/24/2016|       3|NULL|NULL|\n",
      "|       7|    Kelsey|  Cameron|Coffee Wrangler|10/18/2003|       3|NULL|NULL|\n",
      "|       8|  Hamilton|      Emi|Coffee Wrangler|  2/9/2005|       3|NULL|NULL|\n",
      "|       9|  Caldwell|     Veda|Coffee Wrangler|  9/9/2013|       3|NULL|NULL|\n",
      "|      10|       Ima| Winifred|Coffee Wrangler|12/10/2016|       3|NULL|NULL|\n",
      "|      11|      Ruth|   Leslie|  Store Manager| 6/17/2009|       4|NULL|NULL|\n",
      "|      12|  Britanni|   Jorden|Coffee Wrangler| 3/25/2006|       4|NULL|NULL|\n",
      "|      13|      Berk|    Derek|Coffee Wrangler|12/11/2009|       4|NULL|NULL|\n",
      "|      14|     Damon|    Sasha|Coffee Wrangler|  6/5/2010|       4|NULL|NULL|\n",
      "|      15|  Remedios|     Mari|Coffee Wrangler|  5/9/2014|       4|NULL|NULL|\n",
      "|      16|      Reed|      Eve|  Store Manager| 3/30/2006|       5|NULL|NULL|\n",
      "|      17|     Quail|  Octavia|Coffee Wrangler| 12/5/2014|       5|NULL|NULL|\n",
      "|      18|   Ezekiel|   Rashad|Coffee Wrangler|11/13/2005|       5|NULL|NULL|\n",
      "|      19|     Peter|   Paloma|Coffee Wrangler| 3/12/2014|       5|NULL|NULL|\n",
      "|      20|     Ronan|    Magee|Coffee Wrangler| 2/13/2002|       5|NULL|NULL|\n",
      "+--------+----------+---------+---------------+----------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Staff csv\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Read Staff Inventory CSV file') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add flat file path\n",
    "path = r'G:\\Data Science Job and Internship\\Personal Projects\\DW-Analytics-Personal-Project\\Flat-files\\Staff.csv'\n",
    "\n",
    "# Read csv file into Data frame\n",
    "Staff_data = spark.read.csv(path, header = True, inferSchema = True)\n",
    "\n",
    "# Show dataframe schema and data shape\n",
    "Staff_data.printSchema() \n",
    "cd_row = Staff_data.count()\n",
    "cd_cols = len(Staff_data.columns)\n",
    "\n",
    "# See data overview\n",
    "print(cd_row, cd_cols)\n",
    "\n",
    "Staff_data.show()\n",
    "# Stop the Spark session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DW-Analytics-Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
